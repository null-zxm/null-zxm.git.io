---
layout:     post
title:      "LDA and PCA"
subtitle:   " \"Hello World, Hello Blog\""
date:       2017-12-23 12:00:00
author:     "Null"
header-img: "img/post-bg-2015.jpg"
tags:
    - 机器学习
---

> “Yeah It's on. ”

### LDA(线性判断分析)和PCA(主成分分析)

​	刚刚开始我的机器学习之旅,先在西瓜书上面看到LDA这东西,一开始完全搞不明白的,后来查过一些资料之后,又看到了PCA,先在写一篇博客来记录下自己对这两个东西的理解了。

##### LDA

* 在西瓜书上面利用LDA来解决二分类的问题。基本思想很简单,就是将所有的数据都投影到同一个投影方向上面,如何选择这个投影方向就是学习器的目的,那么这个方向向量要如何选择才是最好的呢？
  * 使得不同类别的均值向量在这个投影方向上的投影(中心投影)尽量离开的远
  * 使得每一个类别在这个方向上的投影的‘’宽度‘’尽量小


* 令$u_{i},\sum _{i}$表示为第$i$类的均值向量，协方差矩阵 $w$为这个投影方向的向量

  在直线上的中心投影为 $w^{T}u_{0}和w^{T}u_{1}$

  在直线上面的方差为	$$\sigma _{0}=\frac{1}{N}\sum(w^{T}x_{i}-w^{T}u_{0})(w^{T}x_{i}-w^{T}u_{0}) ^{T}=\frac{1}{N}\sum(w^{T}(x_{i}-u_{0})(x_{i}-u_{0})^{T}w)$$

  又因为$\sum_{i}=\sum\frac{1}{N}(x_{i}-u_{0})(x_{i}-u_{0})^{T}$

  所以$\sigma _{0}=w^{T}\sum_{0}w$

  ​	$  \sigma _{1}=w^{T}\sum_{1}w$

  要让上面要求的大那么只要求出$J$的最大值

  ​		$J=\frac{\left \| w^{T} u_{0}-w^{T}u_{1}\right \|^{2}}{w^{T}\sum_{0}w+w^{T}\sum_{1}w}=\frac{\left \| w^{T} u_{0}-w^{T}u_{1}\right \|^{2}}{w^{T}(\sum_{0}+\sum_{1})w}$

  令$S_{w}=\sum_{0}+\sum_{1}$(类内散度矩阵:就是每个类自己的散度)

  令$Sb=(u_{0}-u_{1})(u_{0}-u_{1})^{T}$(类间散度矩阵:类和类之间的散度)

  那么 $J=\frac{w^{T}S_{b}w}{w^{T}S_{w}w}$

  这个时候注意到了$J$的分子和分母都是$w$的二次那么和$w$的长度无关 那么令$w^{T}S_{w}w=1$只要求$Max w^{T}S_{b}w$由拉格朗日乘子法可得

  $C(w)=w^{T}S_{b}w-\lambda(w^{T}S_{w}w-1)$的极值

  $\Rightarrow \frac{d_{c}}{d_{w}}=2S_{b}w-2\lambda S_{w}w=0$

  $\Rightarrow S_{b}w=\lambda S_{w}w$

  因为$(u_{0}-u_{1})^{T}w$ 是实数所以$S_{b}w=(u_{0}-u_{1})(u_{0}-u_{1})^{T}w=\lambda(u_{0}-u_{1})$

  $\Rightarrow (u_{0}-u_{1})=S_{w}w$

  $\Rightarrow w=S_{w}^{-1}(u_{0}-u_{1})$

  后面可以求出$w$了

  ​

  ​

##### PCA

主成分分析最重要的作用就是选取特征,用来降维的,例如有一个特征所有的样本几乎一样的,那么这个特征就没有价值就可以不要,但是如果直接删了又会造成数据的损失这个时候就可以使用降维技术了,将其他的特征在哪个不大需要的特征的样本形成的空间投影,这样这个特征就消去了,那么应该消去哪个特征呢？

刚刚说了 要消去每个样本的差异很小的特征，换句话来说就是留下方差大的特征,

* 先选一个单位向量$u$做投影,那么这些样本在这个向量上的投影之后的方差

  $\sigma=u^{T}\sum u$

  所以要求的是$\sigma $最大的时候的向量$u$利用拉格朗日乘子法

  $C(u)=u^{T}\sum u+\lambda (1-u^{T}u)$

  $\Rightarrow \frac{d_{c}}{d_{u}}=\sum u-\lambda u=0$

  所以$u$是协方差$\sum$的特征向量$\lambda $对应特征值 $\lambda $越大 得到的方差就越大 所以选取$\sum$的特征值按照大小排列选择前面k个

* 内积与投影

  将内积表示为一种我们熟悉的形式：$A\cdot  B=\left |  A \right |\left |B   \right |cos\theta$

  若B是单位向量的话那么$A \cdot B= \left |  A \right |cos\theta$是A在B上的投影。这就是内积的一种解释，那么推广到多维呢？

  * 看下面的矩阵的点乘$C\cdot D$

    * $\begin{bmatrix}a_{1} &a_{2} \\  b_{1}&b_{2} \end{bmatrix}$$\cdot$$\begin{bmatrix}x_{1} &x_{2}  &x_{3} \\  y_{1}&y_{2}  &y_{3} \end{bmatrix}$=$ \begin{bmatrix}a_{1}x_{1}+a_{2}y_{1} &a_{1}x_{2}+a_{2}y_{2}  &a_{1}x_{3}+a_{2}y_{3}& \\  b_{1}x_{1}+b_{2}y_{1}&b_{1}x_{2}+b_{2}y_{2}  &b_{1}x_{3}+b_{2}y_{3} &\end{bmatrix}$

      上面的式子可以看出来是C的行向量和D的列向量的内积，如果将D的列向量当成基,那么也就是说得到的答案是C的行向量在D的列向量上面的投影（分量）更加抽象表示就是,

    * $\begin{pmatrix}p_{1}\\ p_{2}\\ \vdots \\ p_{R}\end{pmatrix}\begin{pmatrix}a_{1} &a_{2}  &\cdots   & a_{M}\end{pmatrix}=\begin{pmatrix}p_{1}a_{1} &p_{1}a_{2}  &\cdots   &p_{1}a_{M} \\ p_{2}a_{1} &p_{2}a_{2}   &\cdots  &p_{2}a_{M}  \\  \vdots &  \vdots &\ddots   & \vdots \\  p_{R}a_{1}& p_{R}a_{2}  & \cdots & p_{R}a_{M} \end{pmatrix}$

      就是$p$向量在M个基向量上面的投影

      ​

      ​

* 所以PCA就是将$N$个特征投影到选择的$K$个特征上面那么得到的数据就是

  $Data_{M\cdot K}=Data_{M\cdot N}\cdot\sum_{N\cdot K}$